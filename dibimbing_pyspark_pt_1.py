# -*- coding: utf-8 -*-
"""Dibimbing PySpark Pt.1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16nYGoeF1THFXP5S3Q-z6Jqd6NzwWzlNd

# Preparation

## Data Download
"""

#Download datasets
from IPython.display import clear_output

! wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt
! wget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/all/online-retail-dataset.csv

"""## Initialization

Ada 2 pilihan kalau pake `python`, bisa langsung pake `pyspark` library, atau download versi build nya dari web apache-spark nya. Tapi jika mau yg versi build, jangan lupa instalasi JAVA terlebih dahulu
"""

# Install pyspark
!pip install pyspark

"""# SparkContext"""

from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName('dibimbing').setMaster('local[*]')
sc = SparkContext.getOrCreate(conf=conf)
sc

type(sc)

sc.defaultParallelism

# dir(sc)

# help(sc)

sc.version

sc.getConf().getAll()

"""# RDD

## Intro
"""

data = range(1,1001)
len(data)

rdd = sc.parallelize(data)
rdd.getNumPartitions()

rdd

new_rdd = rdd.repartition(10)
new_rdd.getNumPartitions()

newer_rdd = new_rdd.repartition(1)
newer_rdd.getNumPartitions()

newer_rdd.toDebugString()

newer_rdd.take(5)

newer_rdd.takeOrdered(5)

# Part 1 ends here

"""## Transformations"""

for i in range(1,10):
  print(i)

(
  sc
  .parallelize(range(1,10))
  .map(lambda x: x+1)
  .collect()
)

(
  sc
  .parallelize(range(1,10))
  .filter(lambda x: x>5)
  .collect()
)

(
  sc
  .parallelize(range(1,10))
  .map(lambda x: [x,x**2])
  .collect()
)

(
  sc
  .parallelize(range(1,10))
  .flatMap(lambda x: [x,x**2])
  .collect()
)

(
  sc
  .parallelize(range(1,10))
  .flatMap(lambda x: [x,x**2])
  .distinct()
  .collect()
)

"""## Actions"""

# Collect
# Take(n)

(
  sc
  .parallelize(range(1,10))
  .collect()
)

(
  sc
  .parallelize(range(1,10))
  .count()
)

print([i for i in  range(1,10)])

(
  sc
  .parallelize(range(1,10))
  .reduce(lambda x,y: x + y)
)

"""## Key-Value Transformation"""

(
  sc
  .parallelize([('a', 1), ('b', 1), ('b', 2), ('a', 5)])
  .reduceByKey(lambda x,y: x + y)
  .collect()
)

countLetter = sc.parallelize([('a', 1), ('b', 6), ('c', 2), ('a', 5)], 4)
defLetter = sc.parallelize([('a', 'vowel'), ('b', 'consonant'), ('c', 'consonant'), ('d', 'consonant')], 4)
(
    countLetter
    .join(defLetter)
    .map(lambda x: (x[1][1], x[1][0]))
    .reduceByKey(lambda x,y: x + y)
    .collect()
)

"""## Word Count"""

text_data = sc.textFile("/content/t8.shakespeare.txt")

text_data.take(300)

text_with_index = text_data.zipWithIndex()
text_with_index.take(300)

(
  text_data
 .zipWithIndex()
 .map(lambda x: x[1])
 .take(5)
)

proper_text_data = (
  text_data
 .zipWithIndex()
 .filter(lambda x: x[1] >= 246)
 .map(lambda x: x[0])
)

# Gantian gan, gimana caranya supaya data yang dibutuhkan itu adalah text dari line 246 kebawah
#
# Your code here...
#
# Expected output
#'THE SONNETS',
# '',
# 'by William Shakespeare',
# '',
# '',
# '',
# '                     1',
# '  From fairest creatures we desire increase,',
# "  That thereby beauty's rose might never die,",
# '  But as the riper should by time decease,',
# '  His tender heir might bear his memory:',
# '  But thou contracted to thine own bright eyes,',
# "  Feed'st thy light's flame with self-substantial fuel,",
# '  Making a famine where abundance lies,',
# '  Thy self thy foe, to thy sweet self too cruel:',
# "  Thou that art now the world's fresh ornament,", ...

(
  proper_text_data
 .flatMap(lambda x : x.split())
 .take(10)
)

# Now count the occurance of each word, what is the top 5 words ?
#
# Your code here...

(
  proper_text_data
 .flatMap(lambda x : x.split())
 .map(lambda x : (x,1))
 .reduceByKey(lambda x,y: x + y)
 .sortBy(lambda x : x[1],ascending=False)
 .take(10)
)

# Part 2 ends here

"""# Dataframe"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("dibimbingDF").getOrCreate()

(
    spark
 .read
 .text("/content/t8.shakespeare.txt")
 .withColumnRenamed("value", "custom_column")
 .show(10)
)

text_data

from pyspark.sql.types import StringType, StructField, StructType

schema = StructType([StructField("text", StringType(), True)])

(
  spark
 .createDataFrame(text_data.map(lambda x: (x,)), schema)
 .show(10)
)

from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, count, trim

(
    spark.read.text("/content/t8.shakespeare.txt")
    .withColumnRenamed("value", "text")
    .withColumn("words", split(trim("text")," "))
    .withColumn("word", explode("words"))
    .groupBy("word")
    .agg(count("*").alias("count"))
    .orderBy("count",ascending=False)
    .show()
)

raw_data =  [
    (1, '2023-01-01', 'Electronics', 3, 100.0, 'Mobile'),
    (2, '2023-01-01', 'Electronics', 5, 150.0, 'Laptop'),
    (3, '2023-01-02', 'Electronics', 2, 80.0, 'TV'),
    (4, '2023-01-02', 'Fashion', 4, 50.0, 'Shirt'),
    (5, '2023-01-03', 'Electronics', 6, 120.0, 'Headphones'),
    (6, '2023-01-03', 'Fashion', 2, 70.0, 'Jeans'),
    (7, '2023-01-04', 'Fashion', 3, 90.0, 'Dress'),
    (8, '2023-01-04', 'Electronics', 4, 130.0, 'Camera'),
    (9, '2023-01-05', 'Fashion', 5, 110.0, 'Shoes'),
    (10, '2023-01-05', 'Electronics', 2, 70.0, 'Tablet'),
    (11, '2023-01-06', 'Electronics', 4, 120.0, 'Speaker'),
    (12, '2023-01-06', 'Fashion', 3, 80.0, 'Skirt'),
    (13, '2023-01-07', 'Electronics', 6, 150.0, 'Monitor'),
    (14, '2023-01-07', 'Fashion', 2, 70.0, 'Hat'),
    (15, '2023-01-08', 'Electronics', 4, 120.0, 'Mouse'),
    (16, '2023-01-08', 'Fashion', 3, 80.0, 'T-Shirt'),
    (17, '2023-01-09', 'Fashion', 5, 110.0, 'Sunglasses'),
    (18, '2023-01-09', 'Electronics', 2, 70.0, 'Keyboard'),
    (19, '2023-01-10', 'Electronics', 4, 120.0, 'Printer'),
    (20, '2023-01-10', 'Fashion', 3, 80.0, 'Bag'),
    (21, '2023-01-11', 'Home', 2, 50.0, 'Pillow'),
    (22, '2023-01-11', 'Sports', 5, 80.0, 'Tennis racket'),
    (23, '2023-01-12', 'Home', 3, 70.0, 'Curtains'),
    (24, '2023-01-12', 'Sports', 4, 90.0, 'Basketball'),
    (25, '2023-01-13', 'Home', 6, 120.0, 'Candles'),
    (26, '2023-01-13', 'Sports', 2, 60.0, 'Football')
]
column_names= ['id', 'date', 'category', 'quantity', 'price', 'product']

import pandas as pd

retail_pd = pd.DataFrame(raw_data)
retail_pd.columns = column_names
retail_pd['date'] = pd.to_datetime(retail_pd['date'], format="%Y-%m-%d")
retail_pd.dtypes

retail_pd[retail_pd["quantity"] > 5].head()

retail_pd.head()

# Read the data into Spark DataFrame
# Create a DataFrame from the mockup data
spark_df = spark.createDataFrame(raw_data, column_names)

# Selecting columns
spark_selected = spark_df.select('category', 'quantity')
spark_selected.show()

# Filtering rows
# retail_pd[retail_pd["quantity"] > 5]
# spark_filtered = spark_df.filter(spark_df['quantity'] > 5)
spark_filtered = spark_df.where(spark_df['quantity'] > 5)
spark_filtered.show()

# Grouping and aggregating
spark_aggregated = spark_df.groupBy('category').agg({'quantity': 'sum', 'price':'sum'})
spark_aggregated.show()

# Sorting data
spark_sorted = spark_df.sort('quantity')
spark_sorted.show()

# Aggregating functions
spark_aggregated_func = spark_df.agg({'quantity': 'sum', 'price': 'mean'})
spark_aggregated_func.show()

# Applying user-defined functions
spark_df = spark_df.withColumn('total_price', spark_df['quantity'] * spark_df['price'])
spark_df.show()

# Joining DataFrames
df1 = spark_df.select('id', 'category')
df2 = spark_df.select('id', 'price')
spark_joined = df1.join(df2, on='id', how="inner")
spark_joined.show()

# Union DataFrames
df1 = spark_df.where(spark_df['id']<3)
df2 = spark_df.where(spark_df['id']>24)
spark_union = df1.unionAll(df2)
spark_union.show()

# Now what if i want to label quantity_type, < 5 small, 5-10 medium, and >10 large ?
df1 = spark_df.where(spark_df['quantity']<5).withColumn("quantity_type","small")
df2 = spark_df.where(spark_df['id']>24)

import pyspark.sql.functions as F

(
    spark_df
 .withColumn("quantity_type",
             F.when(spark_df.quantity < 5,"Small")
             .when((spark_df.quantity >= 5) & (spark_df.quantity <=10),"Medium")
             .when(spark_df.quantity > 10 ,"Large")
             .otherwise(None)
             )
 .show()
)

# Part 3 ends here

"""# SQL"""

from pyspark.sql.types import StringType, StructField, StructType

# Register the DataFrame as a temporary table/view
schema = StructType([StructField("text", StringType(), True)])

(
    spark
 .createDataFrame(text_data.map(lambda x: (x,)), schema)
 .createOrReplaceTempView("shakespeare")
)

# Perform word count using CTE in Spark SQL
spark.sql("""
    WITH word_table AS (
        SELECT explode(split(trim(text), ' ')) AS word
        FROM shakespeare
    )
    SELECT word, COUNT(*) AS count
    FROM word_table
    GROUP BY word
    ORDER BY COUNT(*) DESC
""").show()

# Register the DataFrame as a temporary table/view
(
  spark_df
 .createOrReplaceTempView("retail")
)

spark.sql(
    """
    select
      *,
      case
        when (quantity <= 5) then 'Small'
        when (quantity > 5) and (quantity < 10) then 'Medium'
        when (quantity > 10) then 'Large'
        else 'None'
      end as quantity_type
    from retail
    """
).show()

from pyspark.sql.functions import expr
(
    spark_df
    .withColumn("quantity_type", expr(
        """
        case
          when (quantity <= 5) then 'Small'
          when (quantity > 5) and (quantity < 10) then 'Medium'
          when (quantity > 10) then 'Large'
          else 'None'
        end
        """
    ))
).show()

# Part 4 ends here

"""# Viz

## Cleanup Date
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("dibimbingDF").getOrCreate()

retail_df = spark.read.csv('online-retail-dataset.csv',header=True)
retail_df.show(10)

retail_df.printSchema()

from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType

schema = StructType([
    StructField("InvoiceNo", StringType(), True),
    StructField("StockCode", StringType(), True),
    StructField("Description", StringType(), True),
    StructField("Quantity", IntegerType(), True),
    StructField("InvoiceDate", TimestampType(), True),
    StructField("UnitPrice", DoubleType(), True),
    StructField("CustomerID", StringType(), True),
    StructField("Country", StringType(), True)
])

retail_df = spark.read.csv('online-retail-dataset.csv',header=True, schema=schema)
retail_df.show(10)

schema = StructType([
    StructField("InvoiceNo", StringType(), True),
    StructField("StockCode", StringType(), True),
    StructField("Description", StringType(), True),
    StructField("Quantity", IntegerType(), True),
    StructField("InvoiceDate", StringType(), True),
    StructField("UnitPrice", DoubleType(), True),
    StructField("CustomerID", StringType(), True),
    StructField("Country", StringType(), True)
])

retail_df = spark.read.csv('online-retail-dataset.csv',header=True, schema=schema)
retail_df.show(10)

from pyspark.sql.functions import to_timestamp, unix_timestamp, split

retail_df_v2 = (
    retail_df
    .withColumn("InvoiceDate", to_timestamp("InvoiceDate", "M/d/yyyy H:mm"))
)

retail_df_v2.show(10)

retail_df_v2.schema

"""## Aggregating"""

from pyspark.sql.functions import date_format

(
    retail_df_v2
    .groupBy(date_format("InvoiceDate", "yyyy-MM-dd").alias("day"))
    .count()
    .show(30)
)

retail_pd = (
    retail_df_v2
    .groupBy(date_format("InvoiceDate", "yyyy-MM-dd").alias("day"))
    .count()
    .toPandas()
)

retail_pd.head()

"""## Plotting"""

retail_pd.plot(x="day",y="count",kind="line")

retail_pd.sort_values(by='day', ascending = False, inplace = True)

retail_pd

import plotly.express as px

fig = px.line(retail_pd, x='day', y='count')
fig.show()

from pyspark.sql.functions import date_format

(
    retail_df_v2
    .groupBy("Country")
    .count()
    .show(10)
)

retail_country_pd = (
    retail_df_v2
    .groupBy("Country")
    .count()
    .toPandas()
)

retail_country_pd.head()

# Plot a pie chart
fig = px.bar(retail_country_pd, y="count", x="Country")
fig.show()

# Diluar UK, mana ya negara lain yang cukup tinggi

# Plot a pie chart
fig = px.bar(retail_country_pd[retail_country_pd['Country'] != 'United Kingdom'], y="count", x="Country")
fig.show()

# Estimasi tokonya buka jam berapa ?
retail_pd_hour = (
    retail_df_v2
    .groupBy(date_format("InvoiceDate", "H").alias("hour"))
    .count()
    .toPandas()
)

retail_pd_hour.info()

retail_pd_hour["hour"] = retail_pd_hour["hour"].astype(int)
retail_pd_hour.sort_values(by='hour', ascending = True, inplace = True)

# kalo tutup jam berapa ?
fig = px.bar(retail_pd_hour, y="count", x="hour")
fig.show()

# Customer paling banyak belanja berapa ? (Dollar & Quantity)
from pyspark.sql.functions import col, sum, round

(
    retail_df_v2
 .withColumn("TotalPrice",col("UnitPrice")*col("Quantity"))
 .groupBy("InvoiceNo","CustomerID")
 .agg(sum('TotalPrice').alias("FullPrice"))
 .withColumn("FullPrice", round("FullPrice",2))
 .orderBy("FullPrice",ascending=False)
 .show(5)
)

#Part 5 ends here

"""#Misc"""

from google.colab.output import eval_js
print(eval_js("google.colab.kernel.proxyPort(4040)"))

# Import SparkSession
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder.master("local[*]").getOrCreate()
# Check Spark Session Information
spark